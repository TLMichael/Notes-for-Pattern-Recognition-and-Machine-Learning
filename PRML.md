# Chapter 2 : Probability Distributions
- **density estimation**: To model the probability distribution p(x) of a random variable x, given a finite set x1, ..., xn of observations.
- **parametric distributions**: Binomial and multinomial distributions for discrete random variables and the Gaussian distribution for continuous random variables.
- **nonparametric density estimation**: These models contain parameters, which controls the model complexity rather than the form of the distribution. Such as histograms, nearest-neighbours, and kernels.

# Chapter 3 : Linear Models for Regression
- **basis function**: The simplest form of linear regression models are also linear functions of the input variables. However, we can obtain a much more useful class of funtions by taking linear combinations of a fixed set of nonlinear functions of input variables, known as *basis functions*.
- **linear models**: Linear models have significant limitations as practical techniques for pattern recognition, particularly for problems involving input spaces of high dimensionality, but they have nice analytical properties and form the foundation for more sophisticated models to be dicussed in later chapters.
- **some basis functions**: Polynomials, spline functions, the sigmoidal basis function, the logistic sigmoid function, the 'tanh' function, the Fourier basis function, wavelets.
- **singular value decomposition**: In linear models, when two or more of the basis vectors are co-linear, or nearly so, the resulting parameter values can have large magnitudes. Such near degeneracies will be very common when dealing with real data sets. The resulting numerical difficulties can be addressed using the technique of SVD.
- **geometrical interpretation of the least-squares solution**: The least-squares regression function is obtained by finding the orthogonal projection of the data vector **t** onto the subspace spanned by the basis function in which each basis function is viewed as a vector of length N.
- **sequential learning**: Sequential algorithms, alse known as on-line algorithms, in which the data points are considered one at a time, and the model parameters updeted after each such presentation. Sequential learning is also appropriate for real-time applications in which the data observations are arriving in a continuous stream, and predictions must be made before all of the data points are seen.
- **bias and variance**: The expected loss can be decomposed into sum of a bias, a variance, and a constant noise term. There is a trade-off between bias and variance. The model with the optimal predictive capability is the one that leads to the best balance between bias and variance.
- **Bayesian treatment of linear regression**: It will avoid over-fitting problem of maximum likelihood, and also lead to automatic methods of determining model complexity using the training data alone.
- **parameter distribution**: A prior probability distribution over the model parameters **w**.
- **predictive distribution**: In practice, we are not usually interested in the value of **w** itself but rather in making predictions of *t* for new values of **x**. This requires that we evaluate the *predictive distribution*.